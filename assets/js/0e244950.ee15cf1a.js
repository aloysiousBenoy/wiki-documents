"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[32199],{3905:(t,e,l)=>{l.d(e,{Zo:()=>d,kt:()=>m});var n=l(67294);function o(t,e,l){return e in t?Object.defineProperty(t,e,{value:l,enumerable:!0,configurable:!0,writable:!0}):t[e]=l,t}function a(t,e){var l=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),l.push.apply(l,n)}return l}function r(t){for(var e=1;e<arguments.length;e++){var l=null!=arguments[e]?arguments[e]:{};e%2?a(Object(l),!0).forEach((function(e){o(t,e,l[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(l)):a(Object(l)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(l,e))}))}return t}function s(t,e){if(null==t)return{};var l,n,o=function(t,e){if(null==t)return{};var l,n,o={},a=Object.keys(t);for(n=0;n<a.length;n++)l=a[n],e.indexOf(l)>=0||(o[l]=t[l]);return o}(t,e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(t);for(n=0;n<a.length;n++)l=a[n],e.indexOf(l)>=0||Object.prototype.propertyIsEnumerable.call(t,l)&&(o[l]=t[l])}return o}var i=n.createContext({}),u=function(t){var e=n.useContext(i),l=e;return t&&(l="function"==typeof t?t(e):r(r({},e),t)),l},d=function(t){var e=u(t.components);return n.createElement(i.Provider,{value:e},t.children)},p="mdxType",c={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},k=n.forwardRef((function(t,e){var l=t.components,o=t.mdxType,a=t.originalType,i=t.parentName,d=s(t,["components","mdxType","originalType","parentName"]),p=u(l),k=o,m=p["".concat(i,".").concat(k)]||p[k]||c[k]||a;return l?n.createElement(m,r(r({ref:e},d),{},{components:l})):n.createElement(m,r({ref:e},d))}));function m(t,e){var l=arguments,o=e&&e.mdxType;if("string"==typeof t||o){var a=l.length,r=new Array(a);r[0]=k;var s={};for(var i in e)hasOwnProperty.call(e,i)&&(s[i]=e[i]);s.originalType=t,s[p]="string"==typeof t?t:o,r[1]=s;for(var u=2;u<a;u++)r[u]=l[u];return n.createElement.apply(null,r)}return n.createElement.apply(null,l)}k.displayName="MDXCreateElement"},72488:(t,e,l)=>{l.r(e),l.d(e,{assets:()=>i,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>u});var n=l(87462),o=(l(67294),l(3905));const a={description:"Deploy YOLOv8 on NVIDIA Jetson using TensorRT",title:"Deploy YOLOv8 on NVIDIA Jetson using TensorRT",keywords:["Edge","reComputer Application"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/YOLOv8-TRT-Jetson",last_update:{date:"07/17/2023",author:"Lakshantha"}},r="Deploy YOLOv8 on NVIDIA Jetson using TensorRT",s={unversionedId:"Edge/reComputer/Application/YOLOv8-TRT-Jetson",id:"Edge/reComputer/Application/YOLOv8-TRT-Jetson",title:"Deploy YOLOv8 on NVIDIA Jetson using TensorRT",description:"Deploy YOLOv8 on NVIDIA Jetson using TensorRT",source:"@site/docs/Edge/reComputer/Application/YOLOv8-TRT-Jetson.md",sourceDirName:"Edge/reComputer/Application",slug:"/YOLOv8-TRT-Jetson",permalink:"/YOLOv8-TRT-Jetson",draft:!1,editUrl:"https://github.com/Seeed-Studio/wiki-documents/blob/docusaurus-version/docs/Edge/reComputer/Application/YOLOv8-TRT-Jetson.md",tags:[],version:"current",lastUpdatedBy:"Lakshantha",lastUpdatedAt:1689552e3,formattedLastUpdatedAt:"Jul 17, 2023",frontMatter:{description:"Deploy YOLOv8 on NVIDIA Jetson using TensorRT",title:"Deploy YOLOv8 on NVIDIA Jetson using TensorRT",keywords:["Edge","reComputer Application"],image:"https://files.seeedstudio.com/wiki/wiki-platform/S-tempor.png",slug:"/YOLOv8-TRT-Jetson",last_update:{date:"07/17/2023",author:"Lakshantha"}},sidebar:"ProductSidebar",previous:{title:"Deploy YOLOv8 on NVIDIA Jetson using TensorRT and DeepStream SDK",permalink:"/YOLOv8-DeepStream-TRT-Jetson"},next:{title:"Getting Started with CVEDIA-RT on NVIDIA\xae Jetson Devices",permalink:"/CVEDIA-Jetson-Getting-Started"}},i={},u=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Flash JetPack to Jetson",id:"flash-jetpack-to-jetson",level:2},{value:"Install Ultralytics Package",id:"install-ultralytics-package",level:2},{value:"Uninstall Torch and Torchvision",id:"uninstall-torch-and-torchvision",level:2},{value:"Install PyTorch and Torchvision",id:"install-pytorch-and-torchvision",level:2},{value:"Use Pre-trained models",id:"use-pre-trained-models",level:2},{value:"Object Detection",id:"object-detection",level:3},{value:"Image Classification",id:"image-classification",level:3},{value:"Image Segmentation",id:"image-segmentation",level:3},{value:"Pose Estimation",id:"pose-estimation",level:3},{value:"Object Tracking",id:"object-tracking",level:3},{value:"Use TensorRT to Improve Inference Speed",id:"use-tensorrt-to-improve-inference-speed",level:2},{value:"Bonus Demo: Exercise Detector and Counter with YOLOv8",id:"bonus-demo-exercise-detector-and-counter-with-yolov8",level:2},{value:"Resources",id:"resources",level:2},{value:"Tech Support &amp; Product Discussion",id:"tech-support--product-discussion",level:2}],d={toc:u};function p(t){let{components:e,...l}=t;return(0,o.kt)("wrapper",(0,n.Z)({},d,l,{components:e,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"deploy-yolov8-on-nvidia-jetson-using-tensorrt"},"Deploy YOLOv8 on NVIDIA Jetson using TensorRT"),(0,o.kt)("p",null,"This wiki guide explains how to deploy a YOLOv8 model into NVIDIA Jetson Platform and perform inference using TensorRT. Here we use TensorRT to maximize the inference performance on the Jetson platform."),(0,o.kt)("p",null,"Different computer vision tasks will be introduced here such as:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Object Detection"),(0,o.kt)("li",{parentName:"ul"},"Image Segmentation"),(0,o.kt)("li",{parentName:"ul"},"Image Classification"),(0,o.kt)("li",{parentName:"ul"},"Pose Estimation"),(0,o.kt)("li",{parentName:"ul"},"Object Tracking")),(0,o.kt)("div",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://files.seeedstudio.com/wiki/YOLOV8-TRT/8.gif\n",style:{width:1e3,height:"auto"}})),(0,o.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Ubuntu Host PC (native or VM using VMware Workstation Player)"),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://www.seeedstudio.com/reComputer-J4012-p-5586.html"},"reComputer Jetson")," or any other NVIDIA Jetson device running JetPack 5.1.1 or higher")),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},"This wiki has been tested and verified on a ",(0,o.kt)("a",{parentName:"p",href:"https://www.seeedstudio.com/reComputer-J4012-p-5586.html"},"reComputer J4012")," powered by NVIDIA Jetson orin NX 16GB module ")),(0,o.kt)("h2",{id:"flash-jetpack-to-jetson"},"Flash JetPack to Jetson"),(0,o.kt)("p",null,"Now you need to make sure that the Jetson device is flashed with a ",(0,o.kt)("a",{parentName:"p",href:"https://developer.nvidia.com/embedded/jetpack"},"JetPack")," system including SDK components such as CUDA, TensorRT, cuDNN and more. You can either use NVIDIA SDK Manager or command-line to flash JetPack to the device."),(0,o.kt)("p",null,"For Seeed Jetson-powered devices flashing guides, please refer to the below links:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://wiki.seeedstudio.com/reComputer_J1010_J101_Flash_Jetpack"},"reComputer J1010 | J101")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://wiki.seeedstudio.com/reComputer_J2021_J202_Flash_Jetpack"},"reComputer J2021 | J202")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://wiki.seeedstudio.com/reComputer_J1020_A206_Flash_JetPack"},"reComputer J1020 | A206")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://wiki.seeedstudio.com/reComputer_J4012_Flash_Jetpack"},"reComputer J4012 | J401")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://wiki.seeedstudio.com/reComputer_A203_Flash_System"},"A203 Carrier Board")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://wiki.seeedstudio.com/reComputer_A205_Flash_System"},"A205 Carrier Board")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://wiki.seeedstudio.com/Jetson_Xavier_AGX_H01_Driver_Installation"},"Jetson Xavier AGX H01 Kit")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://wiki.seeedstudio.com/Jetson_AGX_Orin_32GB_H01_Flash_Jetpack"},"Jetson AGX Orin 32GB H01 Kit"))),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},"Make sure to Flash JetPack version 5.1.1 because that is the version we have verified for this wiki ")),(0,o.kt)("h2",{id:"install-ultralytics-package"},"Install Ultralytics Package"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Step 1.")," Access the terminal of Jetson device, install pip and upgrade it")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"sudo apt update\nsudo apt install -y python3-pip -y\npip3 install --upgrade pip\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Step 2."),"  Install Ultralytics package")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"pip3 install ultralytics\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Step 3."),"  Upgrade numpy version to latest")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"pip3 install numpy -U\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Step 4.")," Reboot the device")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"sudo reboot\n")),(0,o.kt)("h2",{id:"uninstall-torch-and-torchvision"},"Uninstall Torch and Torchvision"),(0,o.kt)("p",null,"The above ultralytics installation will install Torch and Torchvision. However, these 2 packages installed via pip are not compatible to run on Jetson platform wwhich is based on ",(0,o.kt)("strong",{parentName:"p"},"ARM aarch64 architecture"),". Therefore we need to manually install pre-built PyTorch pip wheel and compile/ install Torchvision from source."),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"pip3 uninstall torch torchvision\n")),(0,o.kt)("h2",{id:"install-pytorch-and-torchvision"},"Install PyTorch and Torchvision"),(0,o.kt)("p",null,"Visit ",(0,o.kt)("a",{parentName:"p",href:"https://forums.developer.nvidia.com/t/pytorch-for-jetson"},"this page")," to access all the PyTorch and Torchvision links."),(0,o.kt)("p",null,"Here are some of the versions supported by JetPack 5.0 and above."),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"PyTorch v2.0.0")),(0,o.kt)("p",null,"Supported by JetPack 5.1 (L4T R35.2.1) / JetPack 5.1.1 (L4T R35.3.1) with Python 3.8"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"file_name:")," torch-2.0.0+nv23.05-cp38-cp38-linux_aarch64.whl\n",(0,o.kt)("strong",{parentName:"p"},"URL:")," ",(0,o.kt)("a",{parentName:"p",href:"https://nvidia.box.com/shared/static/i8pukc49h3lhak4kkn67tg9j4goqm0m7.whl"},"https://nvidia.box.com/shared/static/i8pukc49h3lhak4kkn67tg9j4goqm0m7.whl")),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"PyTorch v1.13.0")),(0,o.kt)("p",null,"Supported by JetPack 5.0 (L4T R34.1) / JetPack 5.0.2 (L4T R35.1) / JetPack 5.1 (L4T R35.2.1) / JetPack 5.1.1 (L4T R35.3.1) with Python 3.8"),(0,o.kt)("p",null,(0,o.kt)("strong",{parentName:"p"},"file_name:")," torch-1.13.0a0+d0d6b1f2.nv22.10-cp38-cp38-linux_aarch64.whl\n",(0,o.kt)("strong",{parentName:"p"},"URL:")," ",(0,o.kt)("a",{parentName:"p",href:"https://developer.download.nvidia.com/compute/redist/jp/v502/pytorch/torch-1.13.0a0+d0d6b1f2.nv22.10-cp38-cp38-linux_aarch64.whl"},"https://developer.download.nvidia.com/compute/redist/jp/v502/pytorch/torch-1.13.0a0+d0d6b1f2.nv22.10-cp38-cp38-linux_aarch64.whl")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Step 1.")," Install torch according to your JetPack version in the following format\npip3 ")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"wget <URL> -O <file_name>\npip3 install <file_name>\n")),(0,o.kt)("p",null,"For example, here we are running ",(0,o.kt)("strong",{parentName:"p"},"JP5.1.1")," and therefore we choose ",(0,o.kt)("strong",{parentName:"p"},"PyTorch v2.0.0")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"sudo apt-get install -y libopenblas-base libopenmpi-dev\nwget https://nvidia.box.com/shared/static/i8pukc49h3lhak4kkn67tg9j4goqm0m7.whl -O torch-2.0.0+nv23.05-cp38-cp38-linux_aarch64.whl\npip3 install torch-2.0.0+nv23.05-cp38-cp38-linux_aarch64.whl\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Step 2.")," Install torchvision depending on the version of PyTorch that you have installed. For example, we chose PyTorch v2.0.0, which means, we need to choose Torchvision v0.15.2")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"sudo apt install -y libjpeg-dev zlib1g-dev\ngit clone https://github.com/pytorch/vision torchvision\ncd torchvision\ngit checkout v0.15.2\npython3 setup.py install --user\n")),(0,o.kt)("p",null,"Here is a list of the corresponding torchvision version that you need to install according to the PyTorch version:"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"PyTorch v2.0.0 - torchvision v0.15"),(0,o.kt)("li",{parentName:"ul"},"PyTorch v1.13.0 - torchvision v0.14")),(0,o.kt)("p",null,"If you want a more detailed list, please check ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/pytorch/vision"},"this link"),"."),(0,o.kt)("h2",{id:"use-pre-trained-models"},"Use Pre-trained models"),(0,o.kt)("p",null,"The fastest way to get started with YOLOv8 is to use pre-trained models provided by YOLOv8. However, these are PyTorch models and therefore will only utilize the CPU when inferencing on the Jetson. If you want the best performance of these models on the Jetson while running on the GPU, you can export the PyTorch models to TensorRT by following this section of the wiki."),(0,o.kt)("p",null,"detection - helmet\nclassification - panda\nsegmentation - traffic\npose - exercise\ntracking - people"),(0,o.kt)("h3",{id:"object-detection"},"Object Detection"),(0,o.kt)("p",null,"YOLOv8 offers 5 pre-trained PyTorch model weights for object detection, trained on COCO dataset at input image size 640x640. You can find them below"),(0,o.kt)("table",null,(0,o.kt)("thead",null,(0,o.kt)("tr",null,(0,o.kt)("th",null,"Model"),(0,o.kt)("th",null,"size",(0,o.kt)("br",null),"(pixels)"),(0,o.kt)("th",null,"mAPval",(0,o.kt)("br",null),"50-95"),(0,o.kt)("th",null,"Speed",(0,o.kt)("br",null),"CPU ONNX",(0,o.kt)("br",null),"(ms)"),(0,o.kt)("th",null,"Speed",(0,o.kt)("br",null),"A100 TensorRT",(0,o.kt)("br",null),"(ms)"),(0,o.kt)("th",null,"params",(0,o.kt)("br",null),"(M)"),(0,o.kt)("th",null,"FLOPs",(0,o.kt)("br",null),"(B)"))),(0,o.kt)("tbody",null,(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt",target:"_blank",rel:"noopener noreferrer"},"YOLOv8n")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"37.3"),(0,o.kt)("td",null,"80.4"),(0,o.kt)("td",null,"0.99"),(0,o.kt)("td",null,"3.2"),(0,o.kt)("td",null,"8.7")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt",target:"_blank",rel:"noopener noreferrer"},"YOLOv8s")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"44.9"),(0,o.kt)("td",null,"128.4"),(0,o.kt)("td",null,"1.20"),(0,o.kt)("td",null,"11.2"),(0,o.kt)("td",null,"28.6")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m.pt",target:"_blank",rel:"noopener noreferrer"},"YOLOv8m")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"50.2"),(0,o.kt)("td",null,"234.7"),(0,o.kt)("td",null,"1.83"),(0,o.kt)("td",null,"25.9"),(0,o.kt)("td",null,"78.9")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l.pt",target:"_blank",rel:"noopener noreferrer"},"YOLOv8l")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"52.9"),(0,o.kt)("td",null,"375.2"),(0,o.kt)("td",null,"2.39"),(0,o.kt)("td",null,"43.7"),(0,o.kt)("td",null,"165.2")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x.pt",target:"_blank",rel:"noopener noreferrer"},"YOLOv8x")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"53.9"),(0,o.kt)("td",null,"479.1"),(0,o.kt)("td",null,"3.53"),(0,o.kt)("td",null,"68.2"),(0,o.kt)("td",null,"257.8")))),(0,o.kt)("p",null,"Reference: ",(0,o.kt)("a",{parentName:"p",href:"https://docs.ultralytics.com/tasks/detect"},"https://docs.ultralytics.com/tasks/detect")),(0,o.kt)("p",null,"You can choose and download your desired model from the above table and execute the below command to run inference on an image"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo detect predict model=yolov8n.pt source='https://ultralytics.com/images/bus.jpg' show=True\n")),(0,o.kt)("p",null,"Here for model, you can change to either yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt and it will download the relavant pre-trained model"),(0,o.kt)("p",null,"You can also connect a webcam and execute the below command "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo detect predict model=yolov8n.pt source='0' show=True\n")),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},'If you face any errors when executing the above commands, try adding "device=0" at the end of the command')),(0,o.kt)("div",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://files.seeedstudio.com/wiki/YOLOV8-TRT/2.gif\n",style:{width:1e3,height:"auto"}})),(0,o.kt)("h3",{id:"image-classification"},"Image Classification"),(0,o.kt)("p",null,"YOLOv8 offers 5 pre-trained PyTorch model weights for image classification, trained on ImageNet at input image size 224x224. You can find them below"),(0,o.kt)("table",null,(0,o.kt)("thead",null,(0,o.kt)("tr",null,(0,o.kt)("th",null,"Model"),(0,o.kt)("th",null,"size",(0,o.kt)("br",null),"(pixels)"),(0,o.kt)("th",null,"acc",(0,o.kt)("br",null),"top1"),(0,o.kt)("th",null,"acc",(0,o.kt)("br",null),"top5",(0,o.kt)("br",null)),(0,o.kt)("th",null,"Speed",(0,o.kt)("br",null),"CPU ONNX",(0,o.kt)("br",null),"(ms)",(0,o.kt)("br",null)),(0,o.kt)("th",null,"Speed",(0,o.kt)("br",null),"A100 TensorRT",(0,o.kt)("br",null),"(ms)",(0,o.kt)("br",null),(0,o.kt)("br",null)),(0,o.kt)("th",null,"params",(0,o.kt)("br",null),"(M)",(0,o.kt)("br",null)),(0,o.kt)("th",null,"FLOPs",(0,o.kt)("br",null),"(B) at 640"))),(0,o.kt)("tbody",null,(0,o.kt)("tr",null,(0,o.kt)("td",null,"YOLOv8n-cls"),(0,o.kt)("td",null,"224"),(0,o.kt)("td",null,"66.6"),(0,o.kt)("td",null,"87.0"),(0,o.kt)("td",null,"12.9"),(0,o.kt)("td",null,"0.31"),(0,o.kt)("td",null,"2.7"),(0,o.kt)("td",null,"4.3")),(0,o.kt)("tr",null,(0,o.kt)("td",null,"YOLOv8s-cls"),(0,o.kt)("td",null,"224"),(0,o.kt)("td",null,"72.3"),(0,o.kt)("td",null,"91.1"),(0,o.kt)("td",null,"23.4"),(0,o.kt)("td",null,"0.35"),(0,o.kt)("td",null,"6.4"),(0,o.kt)("td",null,"13.5")),(0,o.kt)("tr",null,(0,o.kt)("td",null,"YOLOv8m-cls"),(0,o.kt)("td",null,"224"),(0,o.kt)("td",null,"76.4"),(0,o.kt)("td",null,"93.2"),(0,o.kt)("td",null,"85.4"),(0,o.kt)("td",null,"0.62"),(0,o.kt)("td",null,"17.0"),(0,o.kt)("td",null,"42.7")),(0,o.kt)("tr",null,(0,o.kt)("td",null,"YOLOv8l-cls"),(0,o.kt)("td",null,"224"),(0,o.kt)("td",null,"78.0"),(0,o.kt)("td",null,"94.1"),(0,o.kt)("td",null,"163.0"),(0,o.kt)("td",null,"0.87"),(0,o.kt)("td",null,"37.5"),(0,o.kt)("td",null,"99.7")),(0,o.kt)("tr",null,(0,o.kt)("td",null," YOLOv8x-cls"),(0,o.kt)("td",null,"224"),(0,o.kt)("td",null,"78.4"),(0,o.kt)("td",null,"94.3"),(0,o.kt)("td",null,"232.0"),(0,o.kt)("td",null,"1.01"),(0,o.kt)("td",null,"57.4"),(0,o.kt)("td",null,"154.8")))),(0,o.kt)("p",null,"Reference: ",(0,o.kt)("a",{parentName:"p",href:"https://docs.ultralytics.com/tasks/classify"},"https://docs.ultralytics.com/tasks/classify")),(0,o.kt)("p",null,"You can choose your desired model and execute the below command to run inference on an image"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo classify predict model=yolov8n-cls.pt source='https://ultralytics.com/images/bus.jpg' show=True\n")),(0,o.kt)("p",null,"Here for model, you can change to either yolov8s-cls.pt, yolov8m-cls.pt, yolov8l-cls.pt, yolov8x-cls.pt and it will download the relavant pre-trained model"),(0,o.kt)("p",null,"You can also connect a webcam and execute the below command "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo classify predict model=yolov8n-cls.pt source='0' show=True\n")),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},'If you face any errors when executing the above commands, try adding "device=0" at the end of the command')),(0,o.kt)("div",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://files.seeedstudio.com/wiki/YOLOV8-TRT/5.gif\n",style:{width:1e3,height:"auto"}})),(0,o.kt)("h3",{id:"image-segmentation"},"Image Segmentation"),(0,o.kt)("p",null,"YOLOv8 offers 5 pre-trained PyTorch model weights for image segmentation, trained on COCO dataset at input image size 640x640. You can find them below"),(0,o.kt)("table",null,(0,o.kt)("thead",null,(0,o.kt)("tr",null,(0,o.kt)("th",null,"Model"),(0,o.kt)("th",null,"size",(0,o.kt)("br",null),"(pixels)"),(0,o.kt)("th",null,"mAPbox",(0,o.kt)("br",null),"50-95"),(0,o.kt)("th",null,"mAPmask",(0,o.kt)("br",null),"50-95"),(0,o.kt)("th",null,"Speed",(0,o.kt)("br",null),"CPU ONNX",(0,o.kt)("br",null),"(ms)"),(0,o.kt)("th",null,"Speed",(0,o.kt)("br",null),"A100 TensorRT",(0,o.kt)("br",null),"(ms)"),(0,o.kt)("th",null,"params",(0,o.kt)("br",null),"(M)"),(0,o.kt)("th",null,"FLOPs",(0,o.kt)("br",null),"(B)"))),(0,o.kt)("tbody",null,(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-seg.pt"},"YOLOv8n-seg")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"36.7"),(0,o.kt)("td",null,"30.5"),(0,o.kt)("td",null,"96.1"),(0,o.kt)("td",null,"1.21"),(0,o.kt)("td",null,"3.4"),(0,o.kt)("td",null,"12.6")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-seg.pt"},"YOLOv8s-seg")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"44.6"),(0,o.kt)("td",null,"36.8"),(0,o.kt)("td",null,"155.7"),(0,o.kt)("td",null,"1.47"),(0,o.kt)("td",null,"11.8"),(0,o.kt)("td",null,"42.6")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-seg.pt"},"YOLOv8m-seg")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"49.9"),(0,o.kt)("td",null,"40.8"),(0,o.kt)("td",null,"317.0"),(0,o.kt)("td",null,"2.18"),(0,o.kt)("td",null,"27.3"),(0,o.kt)("td",null,"110.2")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-seg.pt"},"YOLOv8l-seg")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"52.3"),(0,o.kt)("td",null,"42.6"),(0,o.kt)("td",null,"572.4"),(0,o.kt)("td",null,"2.79"),(0,o.kt)("td",null,"46.0"),(0,o.kt)("td",null,"220.5")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-seg.pt"},"YOLOv8x-seg")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"53.4"),(0,o.kt)("td",null,"43.4"),(0,o.kt)("td",null,"712.1"),(0,o.kt)("td",null,"4.02"),(0,o.kt)("td",null,"71.8"),(0,o.kt)("td",null,"344.1")))),(0,o.kt)("p",null,"Reference: ",(0,o.kt)("a",{parentName:"p",href:"https://docs.ultralytics.com/tasks/segment"},"https://docs.ultralytics.com/tasks/segment")),(0,o.kt)("p",null,"You can choose your desired model and execute the below command to run inference on an image"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo segment predict model=yolov8n-seg.pt source='https://ultralytics.com/images/bus.jpg' show=True\n")),(0,o.kt)("p",null,"Here for model, you can change to either yolov8s-seg.pt, yolov8m-seg.pt, yolov8l-seg.pt, yolov8x-seg.pt and it will download the relavant pre-trained model"),(0,o.kt)("p",null,"You can also connect a webcam and execute the below command "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo segment predict model=yolov8n-seg.pt source='0' show=True\n")),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},'If you face any errors when executing the above commands, try adding "device=0" at the end of the command')),(0,o.kt)("div",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://files.seeedstudio.com/wiki/YOLOV8-TRT/3.gif\n",style:{width:1e3,height:"auto"}})),(0,o.kt)("h3",{id:"pose-estimation"},"Pose Estimation"),(0,o.kt)("p",null,"YOLOv8 offers 6 pre-trained PyTorch model weights for pose estimation, trained on COCO keypoints dataset at input image size 640x640. You can find them below"),(0,o.kt)("table",null,(0,o.kt)("thead",null,(0,o.kt)("tr",null,(0,o.kt)("th",null,"Model"),(0,o.kt)("th",null,"size",(0,o.kt)("br",null),"(pixels)"),(0,o.kt)("th",null,"mAPpose",(0,o.kt)("br",null),"50-95"),(0,o.kt)("th",null,"mAPpose",(0,o.kt)("br",null),"50"),(0,o.kt)("th",null,"Speed",(0,o.kt)("br",null),"CPU ONNX",(0,o.kt)("br",null),"(ms)"),(0,o.kt)("th",null,"Speed",(0,o.kt)("br",null),"A100 TensorRT",(0,o.kt)("br",null),"(ms)"),(0,o.kt)("th",null,"params",(0,o.kt)("br",null),"(M)"),(0,o.kt)("th",null,"FLOPs",(0,o.kt)("br",null),"(B)"))),(0,o.kt)("tbody",null,(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n-pose.pt"},"YOLOv8n-pose")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"50.4"),(0,o.kt)("td",null,"80.1"),(0,o.kt)("td",null,"131.8"),(0,o.kt)("td",null,"1.18"),(0,o.kt)("td",null,"3.3"),(0,o.kt)("td",null,"9.2")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s-pose.pt"},"YOLOv8s-pose")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"60.0"),(0,o.kt)("td",null,"86.2"),(0,o.kt)("td",null,"233.2"),(0,o.kt)("td",null,"1.42"),(0,o.kt)("td",null,"11.6"),(0,o.kt)("td",null,"30.2")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8m-pose.pt"},"YOLOv8m-pose")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"65.0"),(0,o.kt)("td",null,"88.8"),(0,o.kt)("td",null,"456.3"),(0,o.kt)("td",null,"2.00"),(0,o.kt)("td",null,"26.4"),(0,o.kt)("td",null,"81.0")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8l-pose.pt"},"YOLOv8l-pose")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"67.6"),(0,o.kt)("td",null,"90.0"),(0,o.kt)("td",null,"784.5"),(0,o.kt)("td",null,"2.59"),(0,o.kt)("td",null,"44.4"),(0,o.kt)("td",null,"168.6")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose.pt"},"YOLOv8x-pose")),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"69.2"),(0,o.kt)("td",null,"90.2"),(0,o.kt)("td",null,"1607.1"),(0,o.kt)("td",null,"3.73"),(0,o.kt)("td",null,"69.4"),(0,o.kt)("td",null,"263.2")),(0,o.kt)("tr",null,(0,o.kt)("td",null,(0,o.kt)("a",{href:"https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8x-pose-p6.pt"},"YOLOv8x-pose-p6")),(0,o.kt)("td",null,"1280"),(0,o.kt)("td",null,"71.6"),(0,o.kt)("td",null,"91.2"),(0,o.kt)("td",null,"4088.7"),(0,o.kt)("td",null,"10.04"),(0,o.kt)("td",null,"99.1"),(0,o.kt)("td",null,"1066.4")))),(0,o.kt)("p",null,"Reference: ",(0,o.kt)("a",{parentName:"p",href:"https://docs.ultralytics.com/tasks/pose"},"https://docs.ultralytics.com/tasks/pose")),(0,o.kt)("p",null,"You can choose your desired model and execute the below command to run inference on an image"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo pose predict model=yolov8n-pose.pt source='https://ultralytics.com/images/bus.jpg'\n")),(0,o.kt)("p",null,"Here for model, you can change to either yolov8s-pose.pt, yolov8m-pose.pt, yolov8l-pose.pt, yolov8x-pose.pt, yolov8x-pose-p6 and it will download the relavant pre-trained model"),(0,o.kt)("p",null,"You can also connect a webcam and execute the below command "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo pose predict model=yolov8n-pose.pt source='0'\n")),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},'If you face any errors when executing the above commands, try adding "device=0" at the end of the command')),(0,o.kt)("div",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://files.seeedstudio.com/wiki/YOLOV8-TRT/4.gif\n",style:{width:1e3,height:"auto"}})),(0,o.kt)("h3",{id:"object-tracking"},"Object Tracking"),(0,o.kt)("p",null,"Object tracking is a task that involves identifying the location and class of objects, then assigning a unique ID to that detection in video streams."),(0,o.kt)("p",null,"Basically the output of object tracking is the same as object detection with an added object ID."),(0,o.kt)("p",null,"Reference: ",(0,o.kt)("a",{parentName:"p",href:"https://docs.ultralytics.com/modes/track"},"https://docs.ultralytics.com/modes/track")),(0,o.kt)("p",null,"You can choose your desired model based on object detection/ image segmentation and execute the below command to run inference on an video"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},'yolo track model=yolov8n.pt source="https://youtu.be/Zgi9g1ksQHc"\n')),(0,o.kt)("p",null,"Here for model, you can change to either yolov8n.pt, yolov8s.pt, yolov8m.pt, yolov8l.pt, yolov8x.pt, yolov8n-seg.pt, yolov8s-seg.pt, yolov8m-seg.pt, yolov8l-seg.pt, yolov8x-seg.pt, and it will download the relavant pre-trained model"),(0,o.kt)("p",null,"You can also connect a webcam and execute the below command "),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},'yolo track model=yolov8n.pt source="0"\n')),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},'If you face any errors when executing the above commands, try adding "device=0" at the end of the command')),(0,o.kt)("div",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://files.seeedstudio.com/wiki/YOLOV8-TRT/6.gif\n",style:{width:1e3,height:"auto"}})),(0,o.kt)("div",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://files.seeedstudio.com/wiki/YOLOV8-TRT/7.gif\n",style:{width:1e3,height:"auto"}})),(0,o.kt)("h2",{id:"use-tensorrt-to-improve-inference-speed"},"Use TensorRT to Improve Inference Speed"),(0,o.kt)("p",null,"As we mentioned before, if you want to improve the inference speed on the Jetson running YOLOv8 models, you first need to convert the original PyTorch models to TensorRT models. "),(0,o.kt)("p",null,"Follow the steps below to convert YOLOv8 PyTorch models to TensorRT models."),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},"This works for all four computer vision tasks that we have mentioned before")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Step 1.")," Install ONNX which is a requirement")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"pip3 install onnx\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Step 2.")," Downgrade to lower version of Numpy to fix an error")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"pip3 install numpy==1.20.3\n")),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Step 3.")," Execute the export command by specifying the model path")),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo export model=<path_to_pt_file> format=engine device=0\n")),(0,o.kt)("p",null,"For example:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo export model=yolov8n.pt format=engine device=0\n")),(0,o.kt)("admonition",{type:"note"},(0,o.kt)("p",{parentName:"admonition"},"If you, encouter an error about cmake, you can ignore it. Please be patient until the TensorRT export is finished. It might take a few minutes")),(0,o.kt)("p",null,"After the TensorRT model file (.engine) is created, you will see the output as follows"),(0,o.kt)("div",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://files.seeedstudio.com/wiki/YOLOV8-TRT/1.jpg\n",style:{width:800,height:"auto"}})),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("strong",{parentName:"li"},"Step 4.")," If you want to pass additional arguments, you can do so by following the below table")),(0,o.kt)("table",null,(0,o.kt)("thead",null,(0,o.kt)("tr",null,(0,o.kt)("th",null,"Key"),(0,o.kt)("th",null,"Value"),(0,o.kt)("th",null,"Description"))),(0,o.kt)("tbody",null,(0,o.kt)("tr",null,(0,o.kt)("td",null,"imgsz"),(0,o.kt)("td",null,"640"),(0,o.kt)("td",null,"Image size as scalar or (h, w) list, i.e. (640, 480)")),(0,o.kt)("tr",null,(0,o.kt)("td",null,"half"),(0,o.kt)("td",null,"False"),(0,o.kt)("td",null,"FP16 quantization")),(0,o.kt)("tr",null,(0,o.kt)("td",null,"dynamic"),(0,o.kt)("td",null,"False"),(0,o.kt)("td",null,"Dynamic axes")),(0,o.kt)("tr",null,(0,o.kt)("td",null,"simplify"),(0,o.kt)("td",null,"False"),(0,o.kt)("td",null,"Simplify model")),(0,o.kt)("tr",null,(0,o.kt)("td",null,"workspace"),(0,o.kt)("td",null,"4"),(0,o.kt)("td",null,"Workspace size (GB)")))),(0,o.kt)("p",null,"For example, if you want to convert your PyTorch model into a TensorRT model in FP16 quantization, execute as"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo export model=yolov8n.pt format=engine half=True device=0\n")),(0,o.kt)("p",null,"Once the model is exported successfully, you can directly replace this model with ",(0,o.kt)("strong",{parentName:"p"},"model=")," argument inside ",(0,o.kt)("strong",{parentName:"p"},"predict")," command of ",(0,o.kt)("strong",{parentName:"p"},"yolo")," when running all 4 tasks of detection, classification, segmentation, pose estimation."),(0,o.kt)("p",null,"For example, with object detection:"),(0,o.kt)("pre",null,(0,o.kt)("code",{parentName:"pre",className:"language-sh"},"yolo detect predict model=yolov8n.engine source='0' show=True\n")),(0,o.kt)("h2",{id:"bonus-demo-exercise-detector-and-counter-with-yolov8"},"Bonus Demo: Exercise Detector and Counter with YOLOv8"),(0,o.kt)("p",null,"We have built a pose estimation demo application for exercise detection and counting with YOLOv8 using YOLOv8-Pose model. You can check the project ",(0,o.kt)("a",{parentName:"p",href:"https://github.com/yuyoujiang/Exercise-Counter-with-YOLOv8-on-NVIDIA-Jetson"},"here")," to learn more about this demo and deploy on your own Jetson device!"),(0,o.kt)("div",{style:{textAlign:"center"}},(0,o.kt)("img",{src:"https://files.seeedstudio.com/wiki/YOLOV8-TRT/9.gif\n",style:{width:1e3,height:"auto"}})),(0,o.kt)("h2",{id:"resources"},"Resources"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://docs.ultralytics.com"},"YOLOv8 documentation")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://docs.roboflow.com"},"Roboflow documentation")),(0,o.kt)("li",{parentName:"ul"},(0,o.kt)("a",{parentName:"li",href:"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html"},"TensorRT documentation"))),(0,o.kt)("h2",{id:"tech-support--product-discussion"},"Tech Support & Product Discussion"),(0,o.kt)("p",null,"Thank you for choosing our products! We are here to provide you with different support to ensure that your experience with our products is as smooth as possible. We offer several communication channels to cater to different preferences and needs."),(0,o.kt)("div",{class:"button_tech_support_container"},(0,o.kt)("a",{href:"https://forum.seeedstudio.com/",class:"button_forum"}),(0,o.kt)("a",{href:"https://www.seeedstudio.com/contacts",class:"button_email"})),(0,o.kt)("div",{class:"button_tech_support_container"},(0,o.kt)("a",{href:"https://discord.gg/eWkprNDMU7",class:"button_discord"}),(0,o.kt)("a",{href:"https://github.com/Seeed-Studio/wiki-documents/discussions/69",class:"button_discussion"})))}p.isMDXComponent=!0}}]);